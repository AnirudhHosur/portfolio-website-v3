# Production RAG Chatbot

A production-ready Retrieval-Augmented Generation (RAG) chatbot that enables users to interact with their PDF documents using AI. Built with a robust event-driven architecture using Inngest, this system leverages Google's Gemini AI to provide contextually relevant answers based on the content of uploaded PDFs, with efficient vector storage powered by Qdrant.

## ğŸ‘¨â€ğŸ’» About the Developer

**Anirudh Hosur** - Software Developer based in Calgary, AB

With over 3 years of experience in software development, cloud computing, and system design, I specialize in building scalable, high-performance applications using modern technologies. Currently working as a Software Developer at IBM, I focus on delivering innovative multi-cloud solutions.

### ğŸ“ Location
Calgary, Alberta, Canada

### ğŸ”— Connect With Me
- [LinkedIn](https://www.linkedin.com/in/anirudh-hosur-8b924315b/)
- [GitHub](https://github.com/AnirudhHosur)

### ğŸŒ Portfolio Websites
- **Latest Version**: [anirudhhosur.vercel.app](https://anirudhhosur.vercel.app/) (V2)
- **Previous Version**: [anirudhhosur.netlify.app](https://anirudhhosur.netlify.app/) (V1)

## ğŸ® Featured Project: Clue AI Detective Game

[**Live Demo**](https://clue-ai-detective-game-d85a.vercel.app/)

AI-assisted detective game built with Next.js, HeroUI, Tailwind, and Gemini-powered story generation.

### ğŸ¯ Overview

The Clue AI Detective Game is an innovative platform that empowers users to create personalized detective mysteries using artificial intelligence. Players can design their own cases from scratch, incorporating suspects, motives, alibis, and unexpected twists, all generated by advanced AI models. The application combines narrative generation with visual storytelling to create immersive detective experiences that can be shared with friends and the broader community.

### ğŸŒŸ Key Features

- **Dynamic Case Creation Flow**: Guided process from suspect selection to final case summary
- **AI-Powered Storytelling**: Google Gemini orchestration for compelling narratives, motives, and plot twists
- **Visual Evidence Generation**: Replicate-powered clue imagery with custom asynchronous loader
- **Persistent Game Saving**: API routes and Drizzle schema for storing games in Neon/Postgres database
- **Community Exploration**: Explore and play games created by other users in the community
- **Credit-Based System**: Monetization through a credit system with PayPal integration
- **Responsive Design**: Works seamlessly across devices with light/dark mode support
- **Interactive Dashboard**: Central hub for managing created games and credits
- **Interactive Conversations**: Engage in back-and-forth dialogue with AI to shape your mystery
- **Dynamic Story Evolution**: AI remembers context and adapts the narrative based on your choices
- **Multi-Step Gameplay**: Experience 4 distinct conversation phases leading to a meaningful conclusion
- **Life Lessons**: Conclude each game with a thoughtful moral or life lesson

### ğŸ› ï¸ Tech Stack

**Frontend**
- Next.js 14 (Pages Router) with TypeScript
- HeroUI v2 - Modern component library for consistent UI
- Tailwind CSS - Utility-first CSS framework
- Framer Motion - Smooth animations and transitions
- next-themes - Light/dark mode theming

**Backend & APIs**
- Google Gemini - Advanced AI for story generation
- Replicate - AI image generation for visual clues
- Clerk Authentication - Secure user authentication and management
- PayPal Integration - Payment processing for credit purchases
- Firebase Storage - Permanent storage for AI-generated images

**Database & Infrastructure**
- Drizzle ORM - TypeScript ORM with Neon/Postgres support
- Neon Serverless Postgres - Scalable database solution
- Vercel - Deployment and hosting platform

### ğŸ” Security Considerations

**Firebase Configuration Security**
- All Firebase configuration is stored in environment variables
- Firebase operations are performed exclusively on the server-side
- Never expose Firebase credentials to client-side code
- The Firebase service layer runs only on the server

Environment Variables are stored in `.env.local` and excluded from version control via `.gitignore`.

## ğŸš€ Features

- **PDF Processing & Ingestion**: Upload and process PDF documents with automatic text extraction and intelligent chunking
- **Vector Database Storage**: Store document embeddings in Qdrant vector database for lightning-fast semantic similarity search
- **AI-Powered Question Answering**: Use Google Gemini 2.5 Flash to generate accurate, context-aware responses
- **Streamlit UI Interface**: Simple web interface for uploading PDFs and triggering ingestion workflows
- **Modern Portfolio Website**: Beautiful Next.js portfolio showcasing professional experience, skills, and projects
- **Interactive UI Components**: Built with shadcn/ui components and Tailwind CSS for a polished, responsive design
- **Dark/Light Theme Support**: Seamless theme switching with next-themes
- **Animated Transitions**: Smooth animations using framer-motion for enhanced user experience
- **Mobile-First Design**: Fully responsive layout that works across all devices
- **Event-Driven Architecture**: Reliable workflow orchestration using Inngest for scalable processing
- **FastAPI Backend**: High-performance REST API server with real-time event handling
- **Type-Safe Operations**: Pydantic models ensuring data validation and type safety throughout the pipeline

## ğŸ› ï¸ Tech Stack

- **Python 3.12+**: Core development language
- **FastAPI**: High-performance web framework for building APIs
- **Inngest**: Event-driven workflow orchestration platform
- **Qdrant**: Cloud-native vector database for semantic search
- **Google Gemini 2.5 Flash**: State-of-the-art AI model for natural language generation
- **LlamaIndex**: Framework for building RAG applications
- **Sentence Transformers (all-MiniLM-L6-v2)**: Efficient text embedding generation (384 dimensions)
- **Pydantic**: Data validation and settings management
- **Streamlit**: Web application framework for the RAG chat interface
- **Next.js**: React framework for building the portfolio website
- **shadcn/ui**: Component library for consistent, accessible UI components
- **Tailwind CSS**: Utility-first CSS framework for styling
- **Framer Motion**: Animation library for smooth transitions
- **next-themes**: Theme switching library for dark/light mode
- **Uvicorn**: Lightning-fast ASGI server implementation
- **PDF Reader**: PyPDF for reliable PDF text extraction

## ğŸ“‹ Prerequisites

- Python 3.12 or higher
- Google Gemini API Key (from Google AI Studio)
- Qdrant Cloud account or self-hosted Qdrant instance
- Windows, macOS, or Linux operating system

## ğŸš€ Setup

### 1. Clone the repository

```bash
git clone <your-repo-url>
cd Production-RAG-Chatbot
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Set up environment variables

Create a `.env` file in the project root with the following variables:

```env
QDRANT_URL=https://your-qdrant-instance.qdrant.io
QDRANT_API_KEY=your_qdrant_api_key
GEMINI_API_KEY=your_google_gemini_api_key
```

> **Note**: The current project uses Qdrant Cloud. For local development, you can use a self-hosted Qdrant instance by changing the URL accordingly.

### 4. Run the application

Start the FastAPI server:

```bash
uvicorn main:app --reload --port 8000
```

The API will be available at `http://localhost:8000` with interactive documentation at `http://localhost:8000/docs`.

### 5. Access the Streamlit UI (Optional)

In a separate terminal, run the Streamlit interface:

```bash
streamlit run streamlit_app.py
```

The UI will be available at `http://localhost:8501`.

### 6. Run the Portfolio Website (Optional)

Navigate to the frontend directory and start the development server:

```bash
cd frontend
npm install
npm run dev
```

The portfolio website will be available at `http://localhost:3000`.

## ğŸ“ Project Structure

```
Production-RAG-Chatbot/
â”œâ”€â”€ main.py              # FastAPI application with Inngest workflows
â”œâ”€â”€ data_loader.py       # PDF loading, chunking, and embedding functions
â”œâ”€â”€ vector_db.py         # Qdrant vector database operations and client
â”œâ”€â”€ custom_types.py      # Pydantic models for type definitions and validation
â”œâ”€â”€ streamlit_app.py     # Streamlit web interface for PDF upload
â”œâ”€â”€ frontend/            # Next.js portfolio website
â”‚   â”œâ”€â”€ app/             # App router pages and layouts
â”‚   â”œâ”€â”€ components/      # Reusable UI components
â”‚   â”‚   â”œâ”€â”€ ui/          # shadcn/ui components
â”‚   â”‚   â”œâ”€â”€ navbar.tsx   # Navigation component
â”‚   â”‚   â”œâ”€â”€ hero.tsx     # Hero section
â”‚   â”‚   â”œâ”€â”€ about.tsx    # About section
â”‚   â”‚   â”œâ”€â”€ experience.tsx # Experience timeline
â”‚   â”‚   â”œâ”€â”€ skills.tsx   # Skills showcase
â”‚   â”‚   â”œâ”€â”€ projects.tsx # Projects portfolio
â”‚   â”‚   â””â”€â”€ contact.tsx  # Contact section
â”‚   â””â”€â”€ lib/             # Utility functions
â”œâ”€â”€ requirements.txt     # Project dependencies
â”œâ”€â”€ pyproject.toml       # Project configuration (PEP 621)
â”œâ”€â”€ uv.lock             # UV package manager lock file
â”œâ”€â”€ .env                # Environment variables (gitignored)
â”œâ”€â”€ .gitignore          # Git ignore patterns
â””â”€â”€ README.md           # This documentation file
```

## ğŸ“ Project Structure

```
Production-RAG-Chatbot/
â”œâ”€â”€ main.py              # FastAPI application with Inngest workflows
â”œâ”€â”€ data_loader.py       # PDF loading and text embedding functions
â”œâ”€â”€ vector_db.py         # Qdrant vector database operations
â”œâ”€â”€ custom_types.py      # Pydantic models for type definitions
â”œâ”€â”€ requirements.txt     # Project dependencies
â”œâ”€â”€ pyproject.toml       # Project configuration
â””â”€â”€ README.md           # This file
```

## ğŸ¤– Usage

### Via Streamlit UI (Recommended)

1. Run `streamlit run streamlit_app.py`
2. Upload your PDF file through the web interface
3. The ingestion workflow will automatically trigger

### Via API Directly

#### Ingesting PDFs

Send a POST request to trigger the `rag/ingest_pdf` event:

```bash
curl -X POST "http://localhost:8000/api/events" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "rag/ingest_pdf",
    "data": {
      "pdf_path": "/absolute/path/to/your/document.pdf",
      "source_id": "document_v1"
    }
  }'
```

#### Querying PDFs

Send a POST request to trigger the `rag/query_pdf_ai` event:

```bash
curl -X POST "http://localhost:8000/api/events" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "rag/query_pdf_ai",
    "data": {
      "question": "What is this document about?",
      "top_k": 5
    }
  }'
```

### Response Format

Query responses include:
```json
{
  "answer": "Generated answer from Gemini",
  "sources": ["document_v1", "document_v2"],
  "num_contexts": 5
}
```

## ğŸ”§ Configuration

### Qdrant Setup

1. **Cloud Option** (Recommended):
   - Sign up at [Qdrant Cloud](https://cloud.qdrant.io/)
   - Create a new cluster
   - Copy your cluster URL and API key

2. **Local Option**:
   - Install Docker
   - Run: `docker run -p 6333:6333 qdrant/qdrant`
   - Use URL: `http://localhost:6333`
   - No API key needed for local instances

### Gemini Setup

1. Visit [Google AI Studio](https://aistudio.google.com/)
2. Create or select a project
3. Generate an API key
4. Copy the key to your `.env` file

### Environment Variables

```env
# Qdrant Configuration
QDRANT_URL=https://your-cluster-region.cloud.qdrant.io:6334
QDRANT_API_KEY=your-api-key-here

# Gemini Configuration
GEMINI_API_KEY=your-google-gemini-api-key
```

### Advanced Settings

Modify these constants in `data_loader.py` to tune performance:

```python
EMBED_MODEL = "all-MiniLM-L6-v2"  # Embedding model name
EMBED_DIM = 384                   # Must match Qdrant collection dimension
chunk_size = 1000                 # Size of text chunks in characters
chunk_overlap = 0                 # Overlap between chunks
```

### Collection Schema

Qdrant collection is automatically created with:
- **Name**: `docs`
- **Vector Size**: 384 dimensions
- **Distance Metric**: Cosine
- **Payload Fields**: `source` (string), `text` (string)

## ğŸ™ Deployment Options

### Local Development

Standard setup as described above works perfectly for development and testing.

### Production Deployment

#### Option 1: Cloud Platforms

**Render/Vercel**:
- Deploy FastAPI backend
- Set environment variables in dashboard
- Use free tier for small workloads

**Railway**:
- One-click deployment from GitHub
- Automatic environment variable management
- Built-in monitoring

#### Option 2: Container Deployment

Build and run with Docker:

```dockerfile
FROM python:3.12-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### Option 3: Self-Hosted

Deploy on your own server:

1. Set up reverse proxy (Nginx/Apache)
2. Configure SSL certificates
3. Use systemd for process management
4. Set up monitoring and logging

### Scaling Considerations

- **Vertical Scaling**: Increase RAM for larger document collections
- **Horizontal Scaling**: Multiple FastAPI instances behind load balancer
- **Database Scaling**: Qdrant clusters for high availability
- **Rate Limiting**: Implement in FastAPI middleware for production

## ğŸ¤ Contributing

Contributions are welcome! Here's how to get started:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Ensure all tests pass and add new tests if needed
5. Update documentation as necessary
6. Commit your changes (`git commit -m 'feat: add amazing feature'`)
7. Push to the branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

### Development Guidelines

- Follow PEP 8 style guide
- Use type hints for all function signatures
- Write docstrings for public functions
- Keep PRs focused on single features
- Update README.md when adding new features

## ğŸ“Š Performance Metrics

### Typical Benchmarks

- **PDF Processing**: ~2-5 seconds per 10-page document
- **Query Response**: ~1-3 seconds depending on context size
- **Embedding Generation**: ~100ms per chunk (batch processing)
- **Vector Search**: ~50ms for top-5 results

### Optimization Tips

1. **Batch Processing**: Process multiple documents in parallel
2. **Caching**: Cache frequently accessed embeddings
3. **Indexing**: Use Qdrant's HNSW indexing for faster searches
4. **Chunk Size**: Tune chunk size based on document type

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“ Support

If you encounter any issues or have questions:

- File an issue in the GitHub repository
- Check the [Qdrant Documentation](https://qdrant.tech/documentation/)
- Review [Inngest Docs](https://www.inngest.com/docs)
- Consult [FastAPI Documentation](https://fastapi.tiangolo.com/)

## ğŸš€ Future Enhancements

Planned improvements:
- [ ] Support for additional document formats (DOCX, TXT, HTML)
- [ ] Multi-user support with document isolation
- [ ] Conversation history and context retention
- [ ] Customizable chunking strategies
- [ ] Model selection options (Gemini Pro, GPT alternatives)
- [ ] Web search integration for external context
- [ ] Mobile-responsive UI improvements

---

Built with â¤ï¸ using Python, FastAPI, Inngest, Qdrant, and Google Gemini