# Production RAG Chatbot

A production-ready Retrieval-Augmented Generation (RAG) chatbot that enables users to interact with their PDF documents using AI. Built with a robust event-driven architecture using Inngest, this system leverages Google's Gemini AI to provide contextually relevant answers based on the content of uploaded PDFs, with efficient vector storage powered by Qdrant.

## üöÄ Features

- **PDF Processing & Ingestion**: Upload and process PDF documents with automatic text extraction and intelligent chunking
- **Vector Database Storage**: Store document embeddings in Qdrant vector database for lightning-fast semantic similarity search
- **AI-Powered Question Answering**: Use Google Gemini 2.5 Flash to generate accurate, context-aware responses
- **Streamlit UI Interface**: Simple web interface for uploading PDFs and triggering ingestion workflows
- **Event-Driven Architecture**: Reliable workflow orchestration using Inngest for scalable processing
- **FastAPI Backend**: High-performance REST API server with real-time event handling
- **Type-Safe Operations**: Pydantic models ensuring data validation and type safety throughout the pipeline

## üõ†Ô∏è Tech Stack

- **Python 3.12+**: Core development language
- **FastAPI**: High-performance web framework for building APIs
- **Inngest**: Event-driven workflow orchestration platform
- **Qdrant**: Cloud-native vector database for semantic search
- **Google Gemini 2.5 Flash**: State-of-the-art AI model for natural language generation
- **LlamaIndex**: Framework for building RAG applications
- **Sentence Transformers (all-MiniLM-L6-v2)**: Efficient text embedding generation (384 dimensions)
- **Pydantic**: Data validation and settings management
- **Streamlit**: Web application framework for the UI interface
- **Uvicorn**: Lightning-fast ASGI server implementation
- **PDF Reader**: PyPDF for reliable PDF text extraction

## üìã Prerequisites

- Python 3.12 or higher
- Google Gemini API Key (from Google AI Studio)
- Qdrant Cloud account or self-hosted Qdrant instance
- Windows, macOS, or Linux operating system

## üöÄ Setup

### 1. Clone the repository

```bash
git clone <your-repo-url>
cd Production-RAG-Chatbot
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Set up environment variables

Create a `.env` file in the project root with the following variables:

```env
QDRANT_URL=https://your-qdrant-instance.qdrant.io
QDRANT_API_KEY=your_qdrant_api_key
GEMINI_API_KEY=your_google_gemini_api_key
```

> **Note**: The current project uses Qdrant Cloud. For local development, you can use a self-hosted Qdrant instance by changing the URL accordingly.

### 4. Run the application

Start the FastAPI server:

```bash
uvicorn main:app --reload --port 8000
```

The API will be available at `http://localhost:8000` with interactive documentation at `http://localhost:8000/docs`.

### 5. Access the Streamlit UI (Optional)

In a separate terminal, run the Streamlit interface:

```bash
streamlit run streamlit_app.py
```

The UI will be available at `http://localhost:8501`.

## üìÅ Project Structure

```
Production-RAG-Chatbot/
‚îú‚îÄ‚îÄ main.py              # FastAPI application with Inngest workflows
‚îú‚îÄ‚îÄ data_loader.py       # PDF loading, chunking, and embedding functions
‚îú‚îÄ‚îÄ vector_db.py         # Qdrant vector database operations and client
‚îú‚îÄ‚îÄ custom_types.py      # Pydantic models for type definitions and validation
‚îú‚îÄ‚îÄ streamlit_app.py     # Streamlit web interface for PDF upload
‚îú‚îÄ‚îÄ requirements.txt     # Project dependencies
‚îú‚îÄ‚îÄ pyproject.toml       # Project configuration (PEP 621)
‚îú‚îÄ‚îÄ uv.lock             # UV package manager lock file
‚îú‚îÄ‚îÄ .env                # Environment variables (gitignored)
‚îú‚îÄ‚îÄ .gitignore          # Git ignore patterns
‚îî‚îÄ‚îÄ README.md           # This documentation file
```

## üìÅ Project Structure

```
Production-RAG-Chatbot/
‚îú‚îÄ‚îÄ main.py              # FastAPI application with Inngest workflows
‚îú‚îÄ‚îÄ data_loader.py       # PDF loading and text embedding functions
‚îú‚îÄ‚îÄ vector_db.py         # Qdrant vector database operations
‚îú‚îÄ‚îÄ custom_types.py      # Pydantic models for type definitions
‚îú‚îÄ‚îÄ requirements.txt     # Project dependencies
‚îú‚îÄ‚îÄ pyproject.toml       # Project configuration
‚îî‚îÄ‚îÄ README.md           # This file
```

## ü§ñ Usage

### Via Streamlit UI (Recommended)

1. Run `streamlit run streamlit_app.py`
2. Upload your PDF file through the web interface
3. The ingestion workflow will automatically trigger

### Via API Directly

#### Ingesting PDFs

Send a POST request to trigger the `rag/ingest_pdf` event:

```bash
curl -X POST "http://localhost:8000/api/events" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "rag/ingest_pdf",
    "data": {
      "pdf_path": "/absolute/path/to/your/document.pdf",
      "source_id": "document_v1"
    }
  }'
```

#### Querying PDFs

Send a POST request to trigger the `rag/query_pdf_ai` event:

```bash
curl -X POST "http://localhost:8000/api/events" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "rag/query_pdf_ai",
    "data": {
      "question": "What is this document about?",
      "top_k": 5
    }
  }'
```

### Response Format

Query responses include:
```json
{
  "answer": "Generated answer from Gemini",
  "sources": ["document_v1", "document_v2"],
  "num_contexts": 5
}
```

## üîß Configuration

### Qdrant Setup

1. **Cloud Option** (Recommended):
   - Sign up at [Qdrant Cloud](https://cloud.qdrant.io/)
   - Create a new cluster
   - Copy your cluster URL and API key

2. **Local Option**:
   - Install Docker
   - Run: `docker run -p 6333:6333 qdrant/qdrant`
   - Use URL: `http://localhost:6333`
   - No API key needed for local instances

### Gemini Setup

1. Visit [Google AI Studio](https://aistudio.google.com/)
2. Create or select a project
3. Generate an API key
4. Copy the key to your `.env` file

### Environment Variables

```env
# Qdrant Configuration
QDRANT_URL=https://your-cluster-region.cloud.qdrant.io:6334
QDRANT_API_KEY=your-api-key-here

# Gemini Configuration
GEMINI_API_KEY=your-google-gemini-api-key
```

### Advanced Settings

Modify these constants in `data_loader.py` to tune performance:

```python
EMBED_MODEL = "all-MiniLM-L6-v2"  # Embedding model name
EMBED_DIM = 384                   # Must match Qdrant collection dimension
chunk_size = 1000                 # Size of text chunks in characters
chunk_overlap = 0                 # Overlap between chunks
```

### Collection Schema

Qdrant collection is automatically created with:
- **Name**: `docs`
- **Vector Size**: 384 dimensions
- **Distance Metric**: Cosine
- **Payload Fields**: `source` (string), `text` (string)

## üêô Deployment Options

### Local Development

Standard setup as described above works perfectly for development and testing.

### Production Deployment

#### Option 1: Cloud Platforms

**Render/Vercel**:
- Deploy FastAPI backend
- Set environment variables in dashboard
- Use free tier for small workloads

**Railway**:
- One-click deployment from GitHub
- Automatic environment variable management
- Built-in monitoring

#### Option 2: Container Deployment

Build and run with Docker:

```dockerfile
FROM python:3.12-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### Option 3: Self-Hosted

Deploy on your own server:

1. Set up reverse proxy (Nginx/Apache)
2. Configure SSL certificates
3. Use systemd for process management
4. Set up monitoring and logging

### Scaling Considerations

- **Vertical Scaling**: Increase RAM for larger document collections
- **Horizontal Scaling**: Multiple FastAPI instances behind load balancer
- **Database Scaling**: Qdrant clusters for high availability
- **Rate Limiting**: Implement in FastAPI middleware for production

## ü§ù Contributing

Contributions are welcome! Here's how to get started:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Ensure all tests pass and add new tests if needed
5. Update documentation as necessary
6. Commit your changes (`git commit -m 'feat: add amazing feature'`)
7. Push to the branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

### Development Guidelines

- Follow PEP 8 style guide
- Use type hints for all function signatures
- Write docstrings for public functions
- Keep PRs focused on single features
- Update README.md when adding new features

## üìä Performance Metrics

### Typical Benchmarks

- **PDF Processing**: ~2-5 seconds per 10-page document
- **Query Response**: ~1-3 seconds depending on context size
- **Embedding Generation**: ~100ms per chunk (batch processing)
- **Vector Search**: ~50ms for top-5 results

### Optimization Tips

1. **Batch Processing**: Process multiple documents in parallel
2. **Caching**: Cache frequently accessed embeddings
3. **Indexing**: Use Qdrant's HNSW indexing for faster searches
4. **Chunk Size**: Tune chunk size based on document type

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üìû Support

If you encounter any issues or have questions:

- File an issue in the GitHub repository
- Check the [Qdrant Documentation](https://qdrant.tech/documentation/)
- Review [Inngest Docs](https://www.inngest.com/docs)
- Consult [FastAPI Documentation](https://fastapi.tiangolo.com/)

## üöÄ Future Enhancements

Planned improvements:
- [ ] Support for additional document formats (DOCX, TXT, HTML)
- [ ] Multi-user support with document isolation
- [ ] Conversation history and context retention
- [ ] Customizable chunking strategies
- [ ] Model selection options (Gemini Pro, GPT alternatives)
- [ ] Web search integration for external context
- [ ] Mobile-responsive UI improvements

---

Built with ‚ù§Ô∏è using Python, FastAPI, Inngest, Qdrant, and Google Gemini